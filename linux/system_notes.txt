########### Optimizing MaX-THREADS ###########
MAX_THREADS is the maximum number of threads that can be allocated to the HTTP listener for a certain connection.

Compute-intensive tasks:
For N cpus, optimum utilization can almost be guaranteed with a thread pool of N +1.
I/O or other blocking operations:
You can use a larger pool, since not all of the threads will be schedule-able at all times. 

IF N is number of CPU's , U is target utilization , then
Num_threads = N*U*(1+Wait time/Compute time)
Lower limit - TCP/IP sockets are file descriptors, so num_threads will be significantly higher than num_processors due to the I/O.
Higher limit - Consider how much memory JVM uses, how many threads can the OS create (cat /proc/sys/kernel/threads-max), how many file descriptors does the OS allow, what is the target CPU utilization.

In practice, use a test client and generate incremental loads.  Keep increasing MAX_THREADS until reaching the above limits.  Set that as a higher limit to MAX_THREADS. 
Once the limits are determined, it is time to tinker.
Generate load and tweak -Xss to avoid Stack Overflow Exceptions, this will allow further increases to NUM_THREADS.
For really high loads look at increasing number of file descriptors/threads allowed by your OS.


### TCP Buffer Sizes ###
#These should be increased to at least 16MB for 10G paths and tune the autotuning (although buffer bloat now needs to be considered).
sysctl -w net.core.rmem_max=16777216
sysctl -w net.core.wmem_max=16777216
sysctl -w net.ipv4.tcp_rmem="4096 87380 16777216"
sysctl -w net.ipv4.tcp_wmem="4096 16384 16777216"

### Queue Sizes ###
#net.core.somaxconn controls the size of the connection listening queue. The default value of 128 and if you are running a high-volume 
#server and connections are getting refused at a TCP level, then you want to increase this. This is a very tweakable setting in such a
#case. Too high and you'll get resource problems as it tries to notify a server of a large number of connections and many will remain 
#pending, and too low and you'll get refused connections:
sysctl -w net.core.somaxconn=4096
 
#The net.core.netdev_max_backlog controls the size of the incoming packet queue for upper-layer 
#(java) processing. The default (2048) may be increased and other related parameters (TODO MORE EXPLANATION) adjusted with:
sysctl -w net.core.netdev_max_backlog=16384
sysctl -w net.ipv4.tcp_max_syn_backlog=8192
sysctl -w net.ipv4.tcp_syncookies=1
 
### In Load generators ### 
#If many outgoing connections are made (eg on load generators), then the operating system may run low on ports. 
#Thus it is best to increase the port range used and allow reuse of sockets in TIME_WAIT:
sysctl -w net.ipv4.ip_local_port_range="1024 65535"
sysctl -w net.ipv4.tcp_tw_recycle=1
 
### Increase  file descriptor numbers ###
#Busy servers and load generators may run out of file descriptors as the system defaults are normally low.
#These can be increased for a specific user in /etc/security/limits.conf:
#theusername		hard nofile	40000
#theusername		soft nofile	40000

### Congestion Control ###
#linux supports pluggable congestion control algorithms. To get a list of congestion control algorithms that are available in your kernel run:
sysctl net.ipv4.tcp_available_congestion_control
 
#If cubic and/or htcp are not listed then you will need to research the control algorithms for your kernel.
#You can try setting the control to cubic with:
sysctl -w net.ipv4.tcp_congestion_control=cubic
 
### Acceptors >=1 <= # CPUs  ... use #Cores/2 ###
